{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import warnings\n",
    "import logging\n",
    "import hashlib  # Added for hash generation\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from functools import partial\n",
    "import glob\n",
    "\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:47:07,958 - INFO - Snowflake Connector for Python Version: 3.14.0, Python Version: 3.9.6, Platform: macOS-15.4-arm64-arm-64bit\n",
      "2025-04-07 10:47:07,958 - INFO - Connecting to GLOBAL Snowflake domain\n",
      "2025-04-07 10:47:07,959 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://newrelic.okta.com/app/snowflake/exkugjs4xeGHw0Vo10x7/sso/saml?SAMLRequest=lVJdj9owEPwrkfuc2KEgqAWc4OhxUa8tOnKn0jeTbMDFsVOvQ6C%2Fvg4f1bXSndQ3az2zM7uzw5tDqYI9WJRGj0gcMRKAzkwu9WZEntK7cEACdELnQhkNI3IEJDfjIYpSVXxSu61%2BhJ81oAt8I428%2FRiR2mpuBErkWpSA3GV8Ofn8wDsR4wIRrPNy5ELJUXqtrXMVp7Rpmqh5Hxm7oR3GGGUfqEe1kHfkhUT1tkZljTOZUVfKwc%2F0ikRMWbeV8AivsLgQp1KfV%2FCWyvoMQn6fpotw8XWZkmByne7WaKxLsEuwe5nB0%2BPD2QB6BxoaC0pm3kKYCycMRqhNUyixg8yUVe1828i%2FaAE5VWYj%2FbKS2YhUO5lPj8lk3vs0LTar7cds5bJ8t5ZrterNi%2F7%2BW5z%2FMsW6kxYPM51kJHi%2BRttpo00Qa0h0G6jzJdbphawbsn4a93i3z9kgGvR730kw84FKLdyJ%2Ba%2FryOycOLkTVUX%2FGKdw2NWbH9g9wPy%2BYc8mZoc%2BRTS0DYycb4afHNjxf29iSF%2FSL%2Ff3xUeSzBbGNzgGd8aWwr2eWBzFp4rMw%2BIE5VAKqSZ5bgHRJ6eUaW4tCOfP3NkaCB2fVf8%2B9PFv&RelayState=ver%3A1-hint%3A33093663647352978-ETMsDgAAAZYQ7x%2FLABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEEOVFXZVWsVmPNkPxfPs%2B5cAAACA1XuEYJoE8SxE2ESmH2fpp8QoXDk8LUFIqoXuc3T%2BQJv%2BPpOchlG8Qvb31JvxcshJTWAyO3OqrsyqR%2Frb%2Buo2usctF2i93U%2FQB3dqrrBVN7%2BLeu%2FIJnG1qRKZPt0M4GPD9s5WjElvUkYdMQmjEUvyV7PmRUzJ0N8ZoUBU3GuboJsAFMlZt5W3QkrHkjjbqO2lQBOHk8Dr to authenticate...\n",
      "Connected to Snowflake successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_snowflake_connection():\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            authenticator='externalbrowser',\n",
    "            account='newrelicorg-dataos',\n",
    "            # warehouse='APP_TABLEAU_PLATFORM_PROD',\n",
    "            user='XCHENG',\n",
    "        )\n",
    "        print(\"Connected to Snowflake successfully!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Snowflake: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_query(conn, query):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        cur.close()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = create_snowflake_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tasks = \"\"\"\n",
    "with churn_notes as (\n",
    "    select *\n",
    "    exclude (rn)\n",
    "    from (select\n",
    "        \"rpm_account_id\",\n",
    "        \"churn_risk_score\",\n",
    "        date_trunc('month', \"close_date\") close_month,\n",
    "        replace(\n",
    "            \"summary\",\n",
    "            'Please refer to the Portfolio Churn Risk Review Dashboard for more details and comprehensive understanding of key reasons for this account to be flagged risk of churn',\n",
    "            'Please refer to the Portfolio Churn Risk Review Dashboard for more details'\n",
    "        ) \"summary\",\n",
    "        row_number() over (partition by \"rpm_account_id\",\"close_date\" order by \"churn_risk_score\" desc) as rn\n",
    "    from SAGEMAKER_PRODUCTION.DEV_YAN.SLG_PTC_SUMMARY_NEW\n",
    "    )\n",
    "    where rn = 1\n",
    ")\n",
    ", churn AS (\n",
    "    SELECT\n",
    "        f.rpm_account_id AS subscription_account_id,\n",
    "        f.close_month AS close_date,\n",
    "        c.\"summary\" AS churn_notes,\n",
    "        row_number() over (partition by subscription_account_id order by close_date desc) n,\n",
    "        MAX(churn_score) AS churn_risk_score,\n",
    "        CASE\n",
    "            WHEN churn_risk_score >= 0.75 THEN 'High'\n",
    "            WHEN churn_risk_score < 0.75 AND churn_risk_score >= 0.5 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END AS risk_indicator,\n",
    "        max(close_date) over (partition by subscription_account_id) = close_date last_close_date\n",
    "    FROM (\n",
    "        select \n",
    "            \"subscription_rpm_account_id\" rpm_account_id,\n",
    "            date_trunc('month', \"close_week\") close_month,\n",
    "            max(\"churn_risk_score\") churn_score\n",
    "        from sagemaker_production.v3_ptc.\"weekly_ptc_d_summaries\"\n",
    "        group by 1,2 \n",
    "        union all\n",
    "        select\n",
    "            \"rpm_account_id\" rpm_account_id,\n",
    "            \"close_month\" close_month,\n",
    "            max(\"score_mean\") churn_score\n",
    "        from SAGEMAKER_PRODUCTION.DEV_YAN.SLG_PTC_FORECAST\n",
    "        group by 1,2     \n",
    "    ) f\n",
    "    left join churn_notes c on c.\"rpm_account_id\" = f.rpm_account_id and c.close_month = f.close_month\n",
    "    GROUP BY 1, 2, 3\n",
    ")\n",
    ", churned_accounts as (\n",
    "    SELECT\n",
    "        subscription_account_id,\n",
    "        SFDC_ACCOUNT_ID AS SFDC_ID,\n",
    "        SFDC_ACCOUNT_NAME AS name,\n",
    "        act_acr,\n",
    "        churn_indicator,\n",
    "        contract_start_date,\n",
    "        contract_end_date,\n",
    "        buying_program,\n",
    "        report_as_of_dt,\n",
    "        date_trunc('month',report_as_of_dt) as report_month\n",
    "    FROM reporting.consumption_metrics.consumption_daily_metrics\n",
    "    where report_as_of_dt > '2024-04-01'\n",
    "    and sales_hier_geo = 'AMER'\n",
    "    qualify rank() over (partition by subscription_account_id, report_month order by report_as_of_dt desc) = 1\n",
    ")\n",
    ", churn_daily as (\n",
    "    select \n",
    "        c.*\n",
    "    ,   ca.* exclude(report_month, subscription_account_id)\n",
    "    ,   first_value(act_acr) over (partition by ca.sfdc_id order by REPORT_AS_OF_DT) as first_acr\n",
    "    from churn c\n",
    "    inner join churned_accounts ca \n",
    "        on c.subscription_account_id = ca.subscription_account_id\n",
    "        and c.close_date = ca.report_month\n",
    ")\n",
    ", risk_sum as (\n",
    "    select \n",
    "        subscription_account_id\n",
    "    ,   sfdc_id\n",
    "    ,   sum(case when risk_indicator = 'High' then 1 else 0 end) as risk_sum\n",
    "    ,   sum(case when last_close_date = true then 1 else 0 end) as closed\n",
    "    from churn_daily\n",
    "    where buying_program in ('Savings Plan', 'Volume Plan')\n",
    "    group by all\n",
    "    having risk_sum != 0 and closed != 0\n",
    ")\n",
    ", sfdc_filtered as (\n",
    "    select \n",
    "       *\n",
    "    from churn_daily\n",
    "    where sfdc_id in (select distinct sfdc_id from risk_sum)\n",
    ")\n",
    "\n",
    "-- Get top 10 accounts with highest first_acr for each churn_indicator\n",
    ", churned_sfdc as (\n",
    "select \n",
    "    sfdc_id\n",
    ",   first_acr\n",
    ",   sum(case when churn_indicator = 'Y' then 1 else 0 end) as churn\n",
    "from sfdc_filtered\n",
    "group by all\n",
    ")\n",
    "\n",
    ", final_rank as (\n",
    "select \n",
    "    *\n",
    "from (select *, row_number() over (partition by churn order by first_acr desc) as acr_rank from churned_sfdc\n",
    "    )\n",
    ")\n",
    "select \n",
    "   subject, \n",
    "   description,\n",
    "   activity_date,\n",
    "   account_id\n",
    "from conformed.sfdc.stg_sfdc_task\n",
    "where 1=1\n",
    "    and activity_date > '2024-04-01'\n",
    "    and account_id is not null \n",
    "    and account_id in (select distinct sfdc_id from final_rank)\n",
    "    and subject is not null\n",
    "    and description is not null\n",
    "group by all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_churn = \"\"\"\n",
    "with churn_notes as (\n",
    "    select *\n",
    "exclude (rn)\n",
    "from (select\n",
    "    \"rpm_account_id\",\n",
    "    \"churn_risk_score\",\n",
    "    date_trunc('month', \"close_date\") close_month,\n",
    "    replace(\n",
    "        \"summary\",\n",
    "        'Please refer to the Portfolio Churn Risk Review Dashboard for more details and comprehensive understanding of key reasons for this account to be flagged risk of churn',\n",
    "        'Please refer to the Portfolio Churn Risk Review Dashboard for more details'\n",
    "    ) \"summary\",\n",
    "    row_number() over (partition by \"rpm_account_id\",\"close_date\" order by \"churn_risk_score\" desc) as rn\n",
    "from SAGEMAKER_PRODUCTION.DEV_YAN.SLG_PTC_SUMMARY_NEW\n",
    ")\n",
    "where rn = 1\n",
    ")\n",
    "\n",
    ", churn AS (\n",
    "    SELECT\n",
    "        f.rpm_account_id AS subscription_account_id,\n",
    "        f.close_month AS close_date,\n",
    "        c.\"summary\" AS churn_notes,\n",
    "        row_number() over (partition by subscription_account_id order by close_date desc) n,\n",
    "        MAX(churn_score) AS churn_risk_score,\n",
    "        CASE\n",
    "            WHEN churn_risk_score >= 0.75 THEN 'High'\n",
    "            WHEN churn_risk_score < 0.75 AND churn_risk_score >= 0.5 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END AS risk_indicator,\n",
    "        max(close_date) over (partition by subscription_account_id) = close_date last_close_date\n",
    "    FROM (\n",
    "        select \n",
    "            \"subscription_rpm_account_id\" rpm_account_id,\n",
    "            date_trunc('month', \"close_week\") close_month,\n",
    "            max(\"churn_risk_score\") churn_score\n",
    "        from sagemaker_production.v3_ptc.\"weekly_ptc_d_summaries\"\n",
    "        group by 1,2 \n",
    "        union all\n",
    "        select\n",
    "            \"rpm_account_id\" rpm_account_id,\n",
    "            \"close_month\" close_month,\n",
    "            max(\"score_mean\") churn_score\n",
    "        from SAGEMAKER_PRODUCTION.DEV_YAN.SLG_PTC_FORECAST\n",
    "        group by 1,2     \n",
    "    ) f\n",
    "\t\tleft join churn_notes c on c.\"rpm_account_id\" = f.rpm_account_id and c.close_month = f.close_month\n",
    "    GROUP BY 1, 2, 3\n",
    ")\n",
    "\n",
    ", churned_accounts as (\n",
    "    SELECT\n",
    "        subscription_account_id,\n",
    "        SFDC_ACCOUNT_ID AS SFDC_ID,\n",
    "        SFDC_ACCOUNT_NAME AS name,\n",
    "        act_acr,\n",
    "        churn_indicator,\n",
    "        contract_start_date,\n",
    "        contract_end_date,\n",
    "        buying_program,\n",
    "        report_as_of_dt,\n",
    "        date_trunc('month',report_as_of_dt) as report_month\n",
    "    FROM reporting.consumption_metrics.consumption_daily_metrics\n",
    "    where report_as_of_dt > '2024-04-01'\n",
    "    qualify rank() over (partition by subscription_account_id, report_month order by report_as_of_dt desc) = 1\n",
    ")\n",
    "\n",
    "select \n",
    "    c.*\n",
    ",   ca.* exclude(report_month, subscription_account_id)\n",
    "from churn c\n",
    "inner join churned_accounts ca \n",
    "    on c.subscription_account_id = ca.subscription_account_id\n",
    "    and c.close_date = ca.report_month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = execute_query(conn, query_tasks)\n",
    "df_churn = execute_query(conn, query_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_df = df_task.copy()\n",
    "churn_df = df_churn.copy()\n",
    "tasks_df['ACTIVITY_DATE'] = pd.to_datetime(tasks_df['ACTIVITY_DATE'], errors='coerce')\n",
    "churn_df['CLOSE_DATE'] = pd.to_datetime(churn_df['CLOSE_DATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 50  # Smaller batches for better distribution\n",
    "NUM_THREADS = 10  # More threads for I/O bound tasks\n",
    "LLM_MODEL = \"llama3.2:3b\"\n",
    "CATEGORIES = [\n",
    "    \"Support Case Management\",\n",
    "    \"Technical Discussion\",\n",
    "    \"Billing and Invoicing\", \n",
    "    \"Contract and Renewal\",\n",
    "    \"Meeting Scheduling\",\n",
    "    \"Account Management\",\n",
    "    \"Churn Risk Notification\",\n",
    "    \"Event Management and Invitations\",\n",
    "    \"Marketing Communications\",\n",
    "    \"Gift Tracking\",\n",
    "    \"Product Feedback and Strategy\",\n",
    "    \"Brief Acknowledgments\",\n",
    "    \"Call Logs\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "# Global session for Ollama API\n",
    "ollama_session = None\n",
    "\n",
    "def create_ollama_session():\n",
    "    \"\"\"Create a session with connection pooling and retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.5, \n",
    "                   status_forcelist=[429, 500, 502, 503, 504],\n",
    "                   allowed_methods=[\"POST\"])\n",
    "    adapter = HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    return session\n",
    "\n",
    "def normalize_subject(subject):\n",
    "    \"\"\"Normalize email subject line for better matching\"\"\"\n",
    "    if pd.isna(subject) or subject is None:\n",
    "        return \"\"\n",
    "    subject = str(subject).strip()\n",
    "    old_subject = \"\"\n",
    "    while old_subject != subject:\n",
    "        old_subject = subject\n",
    "        subject = re.sub(r'^(re|fwd|fw|forward):\\s*', '', subject, flags=re.IGNORECASE)\n",
    "    subject = re.sub(r'(email|mail):\\s*[<>]+\\s*', '', subject, flags=re.IGNORECASE)\n",
    "    subject = re.sub(r'\\[(inbox|outbox|sent|draft|spam|trash|folder)\\]\\s*-?\\s*', '', subject, flags=re.IGNORECASE)\n",
    "    subject = re.sub(r'\\[external\\]\\s*', '', subject, flags=re.IGNORECASE)\n",
    "    subject = re.sub(r'^[^a-zA-Z0-9]+', '', subject)\n",
    "    subject = re.sub(r'\\s+', ' ', subject).strip()\n",
    "    return subject\n",
    "\n",
    "def extract_top_response(text):\n",
    "    \"\"\"Extract top response from email chains\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Look for common patterns that indicate the beginning of a previous email\n",
    "    separators = [\n",
    "        r\"\\nFrom:.*?\\nSent:\", \n",
    "        r\"\\nFrom:.*?\\n\",\n",
    "        r\"\\nOn .*? wrote:\",\n",
    "        r\"\\n-+\\s*Original Message\\s*-+\",\n",
    "        r\"\\n_{3,}\",\n",
    "        r\"\\nOn .* at .*:\"\n",
    "    ]\n",
    "        \n",
    "    # Join all patterns with OR\n",
    "    combined_pattern = '|'.join(f'({pattern})' for pattern in separators)\n",
    "        \n",
    "    # Find the first occurrence of any separator\n",
    "    match = re.search(combined_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "    if match:\n",
    "        # Extract everything before the first separator\n",
    "        first_part = text[:match.start()].strip()\n",
    "        return first_part\n",
    "        \n",
    "    # If no separator found, return the whole text\n",
    "    return text.strip()\n",
    "\n",
    "def extract_sender(text):\n",
    "    \"\"\"Extract sender from emails\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "        \n",
    "    # Compile regex patterns for better performance\n",
    "    email_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "    from_pattern = re.compile(r\"From:\\s*(.*?)(?:\\[([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\]|<([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})>|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}))\")\n",
    "        \n",
    "    # Find the first section (before any \"Original Message\" text)\n",
    "    separator_pos = text.find(\"Original Message\")\n",
    "    first_section = text[:separator_pos] if separator_pos > 0 else text\n",
    "        \n",
    "    # Try to find a sender from \"From:\" line\n",
    "    from_match = from_pattern.search(first_section)\n",
    "    if from_match:\n",
    "        # Return the first non-None group containing an email\n",
    "        for group in from_match.groups()[1:]:\n",
    "            if group:\n",
    "                return group.strip()\n",
    "                \n",
    "        # Check if the name part contains an email\n",
    "        name_part = from_match.group(1) if from_match.group(1) else \"\"\n",
    "        email_in_name = email_pattern.search(name_part)\n",
    "        if email_in_name:\n",
    "            return email_in_name.group(0).strip()\n",
    "        \n",
    "    # If no match in the \"From:\" line, search for any email address in the text\n",
    "    email_match = email_pattern.search(first_section)\n",
    "    if email_match:\n",
    "        return email_match.group(0).strip()\n",
    "        \n",
    "    return None\n",
    "\n",
    "def generate_hashkey(text):\n",
    "    \"\"\"Generate a hash key from text\"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    return hashlib.md5(text.encode()).hexdigest()[:10]\n",
    "\n",
    "def generate_conversation_id(row):\n",
    "    \"\"\"Generate a conversation ID based on subject and account info\"\"\"\n",
    "    subject = normalize_subject(row.get('SUBJECT', \"\"))\n",
    "    account_id = str(row.get('ACCOUNT_ID', \"unknown\"))\n",
    "    base_data = f\"{account_id}-\"\n",
    "    \n",
    "    # Try to extract organization name and ID from subject\n",
    "    core_topic_match = re.search(r'([A-Za-z-]+)\\s*[\\(\\[]?(\\d+)[\\)\\]]?', subject)\n",
    "    if core_topic_match:\n",
    "        org_name = core_topic_match.group(1).strip()\n",
    "        id_number = core_topic_match.group(2).strip()\n",
    "        topic_key = f\"{base_data}{org_name.lower()}-{id_number}\"\n",
    "        return generate_hashkey(topic_key)\n",
    "    \n",
    "    # Check for meeting patterns\n",
    "    meeting_patterns = [\n",
    "        r'(invitation|accepted|declined|tentative).*?(\\w+\\s*-\\s*\\w+)',\n",
    "        r'(meeting|call|conference|sync).*?(\\w+\\s*-\\s*\\w+)',\n",
    "        r'(zoom|teams|webex|google\\s*meet).*?(\\w+\\s*-\\s*\\w+)'\n",
    "    ]\n",
    "    for pattern in meeting_patterns:\n",
    "        meeting_match = re.search(pattern, subject, re.IGNORECASE)\n",
    "        if meeting_match:\n",
    "            meeting_name = meeting_match.group(2).strip().lower()\n",
    "            return generate_hashkey(f\"{base_data}meeting-{meeting_name}\")\n",
    "    \n",
    "    # If subject exists, use it\n",
    "    if subject:\n",
    "        return generate_hashkey(f\"{base_data}{subject.lower()}\")\n",
    "    \n",
    "    # If TOP_RESPONSE exists, use first 50 chars\n",
    "    if 'TOP_RESPONSE' in row and not pd.isna(row['TOP_RESPONSE']):\n",
    "        text_for_hash = f\"{base_data}{str(row['TOP_RESPONSE'])[:50].lower()}\"\n",
    "        return generate_hashkey(text_for_hash)\n",
    "    \n",
    "    # Final fallback - use activity date\n",
    "    fallback_text = f\"{base_data}{str(row.get('ACTIVITY_DATE', ''))}\"\n",
    "    return generate_hashkey(fallback_text)\n",
    "\n",
    "def calculate_response_time(df):\n",
    "    \"\"\"Calculate response time between messages, calculating conversation position first\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Ensure RESPONSE_TIME column exists\n",
    "    if 'RESPONSE_TIME' not in df_new.columns:\n",
    "        df_new['RESPONSE_TIME'] = np.nan\n",
    "    \n",
    "    # Check if required columns exist for calculation\n",
    "    required_cols = ['ACCOUNT_ID', 'CONVERSATION_ID', 'SENDER', 'ACTIVITY_DATE']\n",
    "    if not all(col in df_new.columns for col in required_cols):\n",
    "        logging.warning(f\"Missing required columns for response time calculation. Have: {list(df_new.columns)}\")\n",
    "        return df_new\n",
    "    \n",
    "    # Sort by account, conversation, and time to establish the natural order\n",
    "    df_new = df_new.sort_values(['ACCOUNT_ID', 'CONVERSATION_ID', 'ACTIVITY_DATE'])\n",
    "    \n",
    "    # Calculate the conversation position within each conversation\n",
    "    df_new['CONVERSATION_POSITION'] = df_new.groupby(['ACCOUNT_ID', 'CONVERSATION_ID']).cumcount() + 1\n",
    "    \n",
    "    # Now calculate response times\n",
    "    for (account_id, conv_id), group in df_new.groupby(['ACCOUNT_ID', 'CONVERSATION_ID']):\n",
    "        if len(group) <= 1:\n",
    "            continue\n",
    "            \n",
    "        # Track the last message time for customer senders\n",
    "        customer_last_time = None\n",
    "        \n",
    "        # Process messages in chronological order\n",
    "        for idx, row in group.iterrows():\n",
    "            sender = row['SENDER']\n",
    "            current_time = row['ACTIVITY_DATE']\n",
    "            \n",
    "            # Skip if missing data\n",
    "            if pd.isna(sender) or pd.isna(current_time) or not isinstance(current_time, (pd.Timestamp, datetime)):\n",
    "                continue\n",
    "            \n",
    "            is_newrelic = '@newrelic.com' in str(sender).lower()\n",
    "            \n",
    "            # Calculate response time only for NewRelic responses to customer messages\n",
    "            if is_newrelic:  # This is a NewRelic sender\n",
    "                if customer_last_time is not None and customer_last_time < current_time:\n",
    "                    # This is a NewRelic response to a customer message\n",
    "                    time_diff = (current_time - customer_last_time).total_seconds() / 3600\n",
    "                    df_new.at[idx, 'RESPONSE_TIME'] = time_diff\n",
    "            else:  # This is a customer sender\n",
    "                customer_last_time = current_time\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def api_call_with_backoff(prompt, retries=5):\n",
    "    \"\"\"Make API call with exponential backoff\"\"\"\n",
    "    global ollama_session\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = ollama_session.post(\n",
    "                'http://localhost:11434/api/generate',\n",
    "                json={\n",
    "                    'model': LLM_MODEL,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'temperature': 0.1\n",
    "                },\n",
    "                timeout=10 + attempt * 5  # Increasing timeout with each retry\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"API error attempt {attempt+1}/{retries}: {str(e)}\")\n",
    "            \n",
    "        # Exponential backoff\n",
    "        if attempt < retries - 1:\n",
    "            sleep_time = 0.5 * (2 ** attempt)  # 0.5, 1, 2, 4, 8...\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    return None  # All retries failed\n",
    "\n",
    "def process_email(row_data):\n",
    "    \"\"\"Process a single email for categorization\"\"\"\n",
    "    idx, row = row_data\n",
    "    \n",
    "    # Get the email content\n",
    "    content = row['TOP_RESPONSE'] if 'TOP_RESPONSE' in row and not pd.isna(row['TOP_RESPONSE']) else \"\"\n",
    "    subject = row['SUBJECT'] if 'SUBJECT' in row and not pd.isna(row['SUBJECT']) else \"\"\n",
    "    \n",
    "    # Clean content for better categorization\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split('\\n')\n",
    "        clean_lines = []\n",
    "        skip_line = False\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip lines with common email metadata patterns\n",
    "            if (re.match(r'^\\s*(From|To|Cc|Bcc|Date|Sent|Subject|Reply-To):', line, re.IGNORECASE) or\n",
    "                re.match(r'^\\s*On .* wrote:', line, re.IGNORECASE) or\n",
    "                re.match(r'^\\s*-+\\s*Original Message\\s*-+', line, re.IGNORECASE) or\n",
    "                re.match(r'^\\s*-{3,}$', line)):\n",
    "                skip_line = True\n",
    "                continue\n",
    "                \n",
    "            # If we see a blank line after metadata, start including content again\n",
    "            if skip_line and not line.strip():\n",
    "                skip_line = False\n",
    "                \n",
    "            if not skip_line:\n",
    "                clean_lines.append(line)\n",
    "        \n",
    "        # Join back into a clean text\n",
    "        clean_content = '\\n'.join(clean_lines).strip()\n",
    "    else:\n",
    "        clean_content = \"\"\n",
    "        \n",
    "    # Normalize the subject\n",
    "    normalized_subject = normalize_subject(subject)\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    Task: You are categorizing business emails.\n",
    "    \n",
    "    Categories (select ONLY ONE from this list):\n",
    "    Support Case Management, Technical Discussion, Billing and Invoicing, Contract and Renewal, Meeting Scheduling, Account Management, Churn Risk Notification, \n",
    "    Event Management and Invitations, Marketing Communications, Gift Tracking, Product Feedback and Strategy, Brief Acknowledgments, Call Logs, Other\n",
    "    \n",
    "    Email subject: {normalized_subject}\n",
    "    \n",
    "    Email content:\n",
    "    {clean_content[:1000]}\n",
    "    \n",
    "    IMPORTANT: Output ONLY ONE category name from the list above.\n",
    "    Do not output any other text, explanation, or create new categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make API call to Ollama with retries\n",
    "    result = api_call_with_backoff(prompt)\n",
    "    other_data = None\n",
    "    \n",
    "    if result:\n",
    "        response_text = result.get('response', '').strip()\n",
    "        \n",
    "        # Normalize and match category\n",
    "        response_lower = response_text.lower()\n",
    "        \n",
    "        # Check for exact matches first\n",
    "        matched = False\n",
    "        for category_name in CATEGORIES:\n",
    "            if category_name.lower() == response_lower:\n",
    "                category = category_name\n",
    "                matched = True\n",
    "                break\n",
    "                \n",
    "        # If no exact match, check for partial matches\n",
    "        if not matched:\n",
    "            for category_name in CATEGORIES:\n",
    "                if category_name.lower() in response_lower:\n",
    "                    category = category_name\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        # If still no match, default to \"Other\"\n",
    "        if not matched:\n",
    "            category = \"Other\"\n",
    "            \n",
    "        # Create data for \"Other\" category if matched\n",
    "        if category == \"Other\":\n",
    "            other_data = {\n",
    "                'SUBJECT': subject,\n",
    "                'TOP_RESPONSE': content[:500] if isinstance(content, str) else \"\",\n",
    "                'LLM_RESPONSE': response_text\n",
    "            }\n",
    "    else:\n",
    "        # API failed completely\n",
    "        category = \"Other\"\n",
    "        other_data = {\n",
    "            'SUBJECT': subject,\n",
    "            'TOP_RESPONSE': content[:500] if isinstance(content, str) else \"\",\n",
    "            'LLM_RESPONSE': \"API Error\"\n",
    "        }\n",
    "    \n",
    "    return idx, category, other_data\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    \"\"\"Process a batch of emails\"\"\"\n",
    "    results = []\n",
    "    others = []\n",
    "    \n",
    "    # Prepare row data tuples for processing\n",
    "    row_data = list(batch_df.iterrows())\n",
    "    \n",
    "    for idx_data in tqdm(row_data, total=len(row_data), desc=\"Processing emails\", leave=False):\n",
    "        idx, category, other_data = process_email(idx_data)\n",
    "        results.append((idx, category))\n",
    "        if other_data:\n",
    "            others.append(other_data)\n",
    "    \n",
    "    return results, others\n",
    "\n",
    "def preprocess_emails(df):\n",
    "    \"\"\"Preprocess emails to extract TOP_RESPONSE and SENDER if needed\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    for col in ['TOP_RESPONSE', 'SENDER']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = None\n",
    "            logging.info(f\"Created column '{col}' in the dataframe\")\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df_new.iterrows(), total=len(df_new), desc=\"Preprocessing emails\"):\n",
    "        # Extract TOP_RESPONSE if not already set\n",
    "        if pd.isna(df_new.at[idx, 'TOP_RESPONSE']) or df_new.at[idx, 'TOP_RESPONSE'] == '':\n",
    "            if 'DESCRIPTION' in row and not pd.isna(row['DESCRIPTION']):\n",
    "                content = row['DESCRIPTION']\n",
    "                top_response = extract_top_response(content)\n",
    "                df_new.at[idx, 'TOP_RESPONSE'] = top_response\n",
    "        \n",
    "        # Extract SENDER if not already set\n",
    "        if pd.isna(df_new.at[idx, 'SENDER']) or df_new.at[idx, 'SENDER'] == '':\n",
    "            if not pd.isna(df_new.at[idx, 'TOP_RESPONSE']):\n",
    "                sender = extract_sender(df_new.at[idx, 'TOP_RESPONSE'])\n",
    "                df_new.at[idx, 'SENDER'] = sender.lower() if sender else None\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def process_with_threading(df_input, num_threads=None, batch_size=None):\n",
    "    \"\"\"\n",
    "    Process emails with threading instead of multiprocessing\n",
    "    \n",
    "    Args:\n",
    "        df_input: Input dataframe\n",
    "        num_threads: Number of threads to use (None for auto)\n",
    "        batch_size: Size of batches for processing (None for default)\n",
    "        \n",
    "    Returns:\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    global ollama_session\n",
    "    \n",
    "    if ollama_session is None:\n",
    "        ollama_session = create_ollama_session()\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # Use default values if not specified\n",
    "    if num_threads is None:\n",
    "        num_threads = NUM_THREADS\n",
    "    if batch_size is None:\n",
    "        batch_size = BATCH_SIZE\n",
    "        \n",
    "    logging.info(f\"Using {num_threads} threads and batch size of {batch_size}\")\n",
    "    \n",
    "    # Initialize required columns if they don't exist\n",
    "    for col in ['CONVERSATION_ID', 'CATEGORY']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "            logging.info(f\"Created column '{col}' in the dataframe\")\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    batches = [df.iloc[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
    "    logging.info(f\"Split data into {num_batches} batches of size {batch_size}\")\n",
    "    \n",
    "    all_results = []\n",
    "    all_others = []\n",
    "    \n",
    "    # Process batches with ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Submit all batches for processing\n",
    "        future_to_batch = {executor.submit(process_batch, batch): i for i, batch in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), \n",
    "                          total=len(batches), desc=\"Processing batches\"):\n",
    "            try:\n",
    "                batch_results, batch_others = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                all_others.extend(batch_others)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing batch: {str(e)}\")\n",
    "    \n",
    "    # Update the dataframe with the results\n",
    "    for idx, category in all_results:\n",
    "        df.at[idx, 'CATEGORY'] = category\n",
    "    \n",
    "    # Save \"Other\" categories\n",
    "    if all_others:\n",
    "        others_df = pd.DataFrame(all_others)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'other_categories_{timestamp}.csv'\n",
    "        others_df.to_csv(filename, index=False)\n",
    "        logging.info(f\"Saved {len(all_others)} 'Other' categorizations to {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def progressive_processing(df_input, chunk_size=5000):\n",
    "    \"\"\"Process large datasets in progressive chunks to avoid memory issues\"\"\"\n",
    "    df = df_input.copy()\n",
    "    total_rows = len(df)\n",
    "    processed_dfs = []\n",
    "    \n",
    "    # Initialize required columns if they don't exist\n",
    "    for col in ['CONVERSATION_ID', 'CATEGORY']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "            logging.info(f\"Created column '{col}' in the dataframe\")\n",
    "    \n",
    "    # Add conversation IDs to all rows (do this once for efficiency)\n",
    "    logging.info(\"Adding conversation IDs...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating conversation IDs\"):\n",
    "        df.at[idx, 'CONVERSATION_ID'] = generate_conversation_id(row)\n",
    "    \n",
    "    # Process in chunks\n",
    "    for start_idx in range(0, total_rows, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, total_rows)\n",
    "        logging.info(f\"Processing chunk {start_idx+1}-{end_idx} of {total_rows}\")\n",
    "        \n",
    "        chunk_df = df.iloc[start_idx:end_idx].copy()\n",
    "        processed_chunk = process_with_threading(chunk_df)\n",
    "        processed_dfs.append(processed_chunk)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        temp_df = pd.concat(processed_dfs, ignore_index=False)\n",
    "        temp_filename = f'processed_emails_intermediate_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        temp_df.to_csv(temp_filename, index=False)\n",
    "        logging.info(f\"Saved intermediate results to {temp_filename}\")\n",
    "    \n",
    "    # Combine all processed chunks\n",
    "    return pd.concat(processed_dfs, ignore_index=False)\n",
    "\n",
    "def improved_email_processing(df_input):\n",
    "    \"\"\"\n",
    "    Main function for improved email processing with optimizations for M1 Pro Max\n",
    "    \n",
    "    Args:\n",
    "        df_input: Input dataframe with email data\n",
    "        \n",
    "    Returns:\n",
    "        Processed dataframe with additional columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize global Ollama session\n",
    "        global ollama_session\n",
    "        ollama_session = create_ollama_session()\n",
    "        \n",
    "        # Check original size\n",
    "        original_row_count = len(df_input)\n",
    "        logging.info(f\"Original dataframe has {original_row_count} rows and columns: {list(df_input.columns)}\")\n",
    "        \n",
    "        # Step 1: Preprocess to extract TOP_RESPONSE and SENDER\n",
    "        logging.info(\"Step 1: Preprocessing emails...\")\n",
    "        df = preprocess_emails(df_input.copy())\n",
    "        \n",
    "        # Step 2: Process emails with progressive approach for large datasets\n",
    "        # or direct threading approach for smaller datasets\n",
    "        if original_row_count > 10000:\n",
    "            logging.info(f\"Large dataset detected ({original_row_count} rows). Using progressive processing...\")\n",
    "            chunk_size = 5000\n",
    "            df = progressive_processing(df, chunk_size=chunk_size)\n",
    "        else:\n",
    "            # Add conversation IDs\n",
    "            logging.info(\"Adding conversation IDs...\")\n",
    "            for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating conversation IDs\"):\n",
    "                df.at[idx, 'CONVERSATION_ID'] = generate_conversation_id(row)\n",
    "            \n",
    "            # Process with threading\n",
    "            logging.info(f\"Processing {len(df)} emails with threading...\")\n",
    "            df = process_with_threading(df, num_threads=NUM_THREADS, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Step 3: Calculate response times\n",
    "        logging.info(\"Calculating response times...\")\n",
    "        df = calculate_response_time(df)\n",
    "        \n",
    "        # Check if we lost any rows\n",
    "        final_row_count = len(df)\n",
    "        if final_row_count != original_row_count:\n",
    "            logging.warning(f\"Row count changed: {original_row_count} -> {final_row_count} ({final_row_count - original_row_count} difference)\")\n",
    "        \n",
    "        # Save the results to a CSV file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        result_filename = f'processed_emails_{timestamp}.csv'\n",
    "        df.to_csv(result_filename, index=False)\n",
    "        logging.info(f\"Results saved to {result_filename}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in email processing: {str(e)}\")\n",
    "        import traceback\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(resume=False, processed_files=None):\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     try:\n",
    "#         # Access the tasks_df from the global scope\n",
    "#         global tasks_df\n",
    "#         if 'tasks_df' not in globals():\n",
    "#             logging.error(\"tasks_df not found. Please load the dataframe before running.\")\n",
    "#             return None\n",
    "        \n",
    "#         # Option to resume processing\n",
    "#         if resume:\n",
    "#             logging.info(\"Resuming from interrupted processing...\")\n",
    "            \n",
    "#             if processed_files is None:\n",
    "#                 # Default files if none provided\n",
    "#                 csv_path = './'\n",
    "#                 processed_files = glob.glob(os.path.join(csv_path, 'processed_emails_intermediate_*.csv'))\n",
    "\n",
    "            \n",
    "#             # Import the function directly from the module - FIXED LINE\n",
    "#             from filter_processed_rows import filter_processed_rows\n",
    "            \n",
    "#             # Now call the function directly - FIXED LINE\n",
    "#             filtered_df = filter_processed_rows(tasks_df, processed_files)\n",
    "            \n",
    "#             if filtered_df is not None and len(filtered_df) > 0:\n",
    "#                 # Update tasks_df to only contain unprocessed rows\n",
    "#                 tasks_df = filtered_df\n",
    "#                 logging.info(f\"Updated tasks_df to contain only {len(filtered_df)} unprocessed rows\")\n",
    "                \n",
    "#                 # Process the remaining rows\n",
    "#                 processed_df = improved_email_processing(tasks_df)\n",
    "#                 return processed_df\n",
    "#             else:\n",
    "#                 logging.info(\"No unprocessed rows to process or error in filtering\")\n",
    "#                 return None\n",
    "        \n",
    "#         # Otherwise, process all emails from scratch\n",
    "#         processed_df = improved_email_processing(tasks_df)\n",
    "        \n",
    "#         # Replace the global tasks_df with the processed version\n",
    "#         tasks_df = processed_df\n",
    "        \n",
    "#         return processed_df\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error in main function: {str(e)}\")\n",
    "#         import traceback\n",
    "#         logging.error(traceback.format_exc())\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:48:08,140 - INFO - Resuming from interrupted processing...\n",
      "2025-04-07 10:48:08,142 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_200232.csv\n",
      "2025-04-07 10:48:08,956 - INFO - Loaded 20000 processed rows from ./processed_emails_intermediate_20250404_200232.csv\n",
      "2025-04-07 10:48:08,957 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_164724.csv\n",
      "2025-04-07 10:48:09,821 - INFO - Loaded 20000 processed rows from ./processed_emails_intermediate_20250404_164724.csv\n",
      "2025-04-07 10:48:09,822 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_184509.csv\n",
      "2025-04-07 10:48:10,641 - INFO - Loaded 20000 processed rows from ./processed_emails_intermediate_20250405_184509.csv\n",
      "2025-04-07 10:48:10,642 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_192927.csv\n",
      "2025-04-07 10:48:11,291 - INFO - Loaded 15000 processed rows from ./processed_emails_intermediate_20250404_192927.csv\n",
      "2025-04-07 10:48:11,292 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_192634.csv\n",
      "2025-04-07 10:48:12,505 - INFO - Loaded 27182 processed rows from ./processed_emails_intermediate_20250405_192634.csv\n",
      "2025-04-07 10:48:12,506 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_172119.csv\n",
      "2025-04-07 10:48:13,522 - INFO - Loaded 25000 processed rows from ./processed_emails_intermediate_20250404_172119.csv\n",
      "2025-04-07 10:48:13,523 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_182228.csv\n",
      "2025-04-07 10:48:13,736 - INFO - Loaded 5000 processed rows from ./processed_emails_intermediate_20250404_182228.csv\n",
      "2025-04-07 10:48:13,737 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_181607.csv\n",
      "2025-04-07 10:48:14,386 - INFO - Loaded 15000 processed rows from ./processed_emails_intermediate_20250405_181607.csv\n",
      "2025-04-07 10:48:14,386 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_120711.csv\n",
      "2025-04-07 10:48:14,596 - INFO - Loaded 5000 processed rows from ./processed_emails_intermediate_20250404_120711.csv\n",
      "2025-04-07 10:48:14,596 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_211011.csv\n",
      "2025-04-07 10:48:15,966 - INFO - Loaded 139173 processed rows from ./processed_emails_intermediate_20250404_211011.csv\n",
      "2025-04-07 10:48:15,966 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_185557.csv\n",
      "2025-04-07 10:48:16,377 - INFO - Loaded 10000 processed rows from ./processed_emails_intermediate_20250404_185557.csv\n",
      "2025-04-07 10:48:16,377 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_174703.csv\n",
      "2025-04-07 10:48:16,785 - INFO - Loaded 10000 processed rows from ./processed_emails_intermediate_20250405_174703.csv\n",
      "2025-04-07 10:48:16,785 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_191407.csv\n",
      "2025-04-07 10:48:17,800 - INFO - Loaded 25000 processed rows from ./processed_emails_intermediate_20250405_191407.csv\n",
      "2025-04-07 10:48:17,800 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_203552.csv\n",
      "2025-04-07 10:48:18,949 - INFO - Loaded 134173 processed rows from ./processed_emails_intermediate_20250404_203552.csv\n",
      "2025-04-07 10:48:18,949 - INFO - Loading processed data from ./processed_emails_intermediate_20250405_171748.csv\n",
      "2025-04-07 10:48:19,152 - INFO - Loaded 5000 processed rows from ./processed_emails_intermediate_20250405_171748.csv\n",
      "2025-04-07 10:48:19,152 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_124134.csv\n",
      "2025-04-07 10:48:19,570 - INFO - Loaded 10000 processed rows from ./processed_emails_intermediate_20250404_124134.csv\n",
      "2025-04-07 10:48:19,570 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_150529.csv\n",
      "2025-04-07 10:48:19,775 - INFO - Loaded 5000 processed rows from ./processed_emails_intermediate_20250404_150529.csv\n",
      "2025-04-07 10:48:19,776 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_161337.csv\n",
      "2025-04-07 10:48:20,479 - INFO - Loaded 15000 processed rows from ./processed_emails_intermediate_20250404_161337.csv\n",
      "2025-04-07 10:48:20,479 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_154004.csv\n",
      "2025-04-07 10:48:20,953 - INFO - Loaded 10000 processed rows from ./processed_emails_intermediate_20250404_154004.csv\n",
      "2025-04-07 10:48:20,953 - INFO - Loading processed data from ./processed_emails_intermediate_20250404_214329.csv\n",
      "2025-04-07 10:48:22,585 - INFO - Loaded 144173 processed rows from ./processed_emails_intermediate_20250404_214329.csv\n",
      "2025-04-07 10:48:24,925 - INFO - Combined processed data has 97185 unique rows\n",
      "2025-04-07 10:48:24,936 - INFO - Original dataframe has 97360 rows\n",
      "2025-04-07 10:48:24,936 - INFO - Processed dataframe has 97185 rows\n",
      "2025-04-07 10:48:24,937 - INFO - Using composite hash key from columns: ['ACCOUNT_ID', 'ACTIVITY_DATE', 'SUBJECT', 'DESCRIPTION']\n",
      "2025-04-07 10:48:24,944 - INFO - Converted ACTIVITY_DATE in processed data to datetime\n",
      "2025-04-07 10:48:24,944 - INFO - Generating hash keys for processed data...\n",
      "2025-04-07 10:48:29,743 - INFO - Generated 97183 unique hash keys from processed data\n",
      "2025-04-07 10:48:29,743 - INFO - Identifying unprocessed rows...\n",
      "Identifying unprocessed rows: 100%|██████████| 97360/97360 [00:04<00:00, 19688.06it/s]\n",
      "2025-04-07 10:48:34,690 - INFO - Remaining unprocessed rows: 170 (removed 97190 rows)\n",
      "2025-04-07 10:48:34,691 - WARNING - Note: Removed 97190 rows but processed data has 97185 rows\n",
      "2025-04-07 10:48:34,691 - INFO - This may be due to duplicate entries when comparing only the key columns\n",
      "2025-04-07 10:48:34,843 - INFO - Updated tasks_df to contain only 170 unprocessed rows\n",
      "2025-04-07 10:48:34,844 - INFO - Original dataframe has 170 rows and columns: ['SUBJECT', 'DESCRIPTION', 'ACTIVITY_DATE', 'ACCOUNT_ID']\n",
      "2025-04-07 10:48:34,844 - INFO - Step 1: Preprocessing emails...\n",
      "2025-04-07 10:48:34,845 - INFO - Created column 'TOP_RESPONSE' in the dataframe\n",
      "2025-04-07 10:48:34,845 - INFO - Created column 'SENDER' in the dataframe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bef84a0572409381648e2d0e69fc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing emails:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:48:34,876 - INFO - Adding conversation IDs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1adcc3464b40b38ff3bcea9e765856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating conversation IDs:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:48:34,893 - INFO - Processing 170 emails with threading...\n",
      "2025-04-07 10:48:34,894 - INFO - Using 10 threads and batch size of 50\n",
      "2025-04-07 10:48:34,894 - INFO - Created column 'CATEGORY' in the dataframe\n",
      "2025-04-07 10:48:34,895 - INFO - Split data into 4 batches of size 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad5f71bb43444ba35a9cb872fe8a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing emails:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f980450609fa4845823faf6812f8164e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing emails:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e835187ff85845d5a4642ca1857b4e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing emails:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c6f3f47a524f1db53fd61bedf52f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing emails:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100d62783f2b4d2c856796317413bb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:49:32,991 - INFO - Calculating response times...\n",
      "2025-04-07 10:49:32,992 - WARNING - Missing required columns for response time calculation. Have: ['SUBJECT', 'DESCRIPTION', 'ACTIVITY_DATE', 'ACCOUNT_ID', 'TOP_RESPONSE', 'SENDER', 'CONVERSATION_ID', 'CATEGORY', 'RESPONSE_TIME']\n",
      "2025-04-07 10:49:33,014 - INFO - Results saved to processed_emails_20250407_104932.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>ACTIVITY_DATE</th>\n",
       "      <th>ACCOUNT_ID</th>\n",
       "      <th>TOP_RESPONSE</th>\n",
       "      <th>SENDER</th>\n",
       "      <th>CONVERSATION_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>RESPONSE_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>Email: New Relic support case update: Case #00...</td>\n",
       "      <td>Additional To: jaiprasad.arsikere@alight.com\\n...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8qObQAJ</td>\n",
       "      <td>Additional To: jaiprasad.arsikere@alight.com\\n...</td>\n",
       "      <td>jaiprasad.arsikere@alight.com</td>\n",
       "      <td>785d5e7a39</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>Email: New Relic support case update: Case #00...</td>\n",
       "      <td>Additional To: felipe.braun@patientpoint.com\\n...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8a8tQAB</td>\n",
       "      <td>Additional To: felipe.braun@patientpoint.com\\n...</td>\n",
       "      <td>felipe.braun@patientpoint.com</td>\n",
       "      <td>7926649e69</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918</th>\n",
       "      <td>Email: New Relic support case update: Case #00...</td>\n",
       "      <td>Additional To: harpreet_singh12@optum.com\\nCC:...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S92XeQAJ</td>\n",
       "      <td>Additional To: harpreet_singh12@optum.com\\nCC:...</td>\n",
       "      <td>harpreet_singh12@optum.com</td>\n",
       "      <td>b9787916e5</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>Email: New Relic support case created: Case #0...</td>\n",
       "      <td>Additional To: akumar5@bjs.com\\nCC: samyx@newr...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S91CEQAZ</td>\n",
       "      <td>Additional To: akumar5@bjs.com\\nCC: samyx@newr...</td>\n",
       "      <td>akumar5@bjs.com</td>\n",
       "      <td>4de2ce8f35</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10484</th>\n",
       "      <td>Email: Unlocking Infra &amp; Logs in New Relic</td>\n",
       "      <td>https://app.salesloft.com/app/emails/detail/24...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001mXXwSQAW</td>\n",
       "      <td>https://app.salesloft.com/app/emails/detail/24...</td>\n",
       "      <td>chris.lewicki@meridianlink.com</td>\n",
       "      <td>bc7730854a</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96146</th>\n",
       "      <td>Email: How Other Teams Are Maximizing Infra &amp; ...</td>\n",
       "      <td>https://app.salesloft.com/app/emails/detail/24...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8xXvQAJ</td>\n",
       "      <td>https://app.salesloft.com/app/emails/detail/24...</td>\n",
       "      <td>jimmy.caroupapoulle@on24.com</td>\n",
       "      <td>9201a4f7a9</td>\n",
       "      <td>Product Feedback and Strategy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96175</th>\n",
       "      <td>Email: RE: [EXTERNAL] - RE: New Relic support ...</td>\n",
       "      <td>Additional To: rmedida@opentext.com\\nCC: mcast...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8bvlQAB</td>\n",
       "      <td>Additional To: rmedida@opentext.com\\nCC: mcast...</td>\n",
       "      <td>rmedida@opentext.com</td>\n",
       "      <td>563b0afab1</td>\n",
       "      <td>Account Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96359</th>\n",
       "      <td>Email: New Relic support case comment: Case #0...</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8XGAQA3</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>john.stauffer@usfoods.com</td>\n",
       "      <td>279f03981e</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96360</th>\n",
       "      <td>Email: New Relic support case closed #00257871...</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8XGAQA3</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>john.stauffer@usfoods.com</td>\n",
       "      <td>279f03981e</td>\n",
       "      <td>Account Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96371</th>\n",
       "      <td>Email: RE: New Relic support case created: Cas...</td>\n",
       "      <td>Additional To: support-systems+newrelic2@acqui...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0011U00001S8hX9QAJ</td>\n",
       "      <td>Additional To: support-systems+newrelic2@acqui...</td>\n",
       "      <td>support-systems+newrelic2@acquia.com</td>\n",
       "      <td>3dbdc58c08</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SUBJECT  \\\n",
       "9916   Email: New Relic support case update: Case #00...   \n",
       "9917   Email: New Relic support case update: Case #00...   \n",
       "9918   Email: New Relic support case update: Case #00...   \n",
       "9919   Email: New Relic support case created: Case #0...   \n",
       "10484         Email: Unlocking Infra & Logs in New Relic   \n",
       "...                                                  ...   \n",
       "96146  Email: How Other Teams Are Maximizing Infra & ...   \n",
       "96175  Email: RE: [EXTERNAL] - RE: New Relic support ...   \n",
       "96359  Email: New Relic support case comment: Case #0...   \n",
       "96360  Email: New Relic support case closed #00257871...   \n",
       "96371  Email: RE: New Relic support case created: Cas...   \n",
       "\n",
       "                                             DESCRIPTION ACTIVITY_DATE  \\\n",
       "9916   Additional To: jaiprasad.arsikere@alight.com\\n...    2025-04-07   \n",
       "9917   Additional To: felipe.braun@patientpoint.com\\n...    2025-04-07   \n",
       "9918   Additional To: harpreet_singh12@optum.com\\nCC:...    2025-04-07   \n",
       "9919   Additional To: akumar5@bjs.com\\nCC: samyx@newr...    2025-04-07   \n",
       "10484  https://app.salesloft.com/app/emails/detail/24...    2025-04-07   \n",
       "...                                                  ...           ...   \n",
       "96146  https://app.salesloft.com/app/emails/detail/24...    2025-04-07   \n",
       "96175  Additional To: rmedida@opentext.com\\nCC: mcast...    2025-04-07   \n",
       "96359  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-07   \n",
       "96360  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-07   \n",
       "96371  Additional To: support-systems+newrelic2@acqui...    2025-04-07   \n",
       "\n",
       "               ACCOUNT_ID                                       TOP_RESPONSE  \\\n",
       "9916   0011U00001S8qObQAJ  Additional To: jaiprasad.arsikere@alight.com\\n...   \n",
       "9917   0011U00001S8a8tQAB  Additional To: felipe.braun@patientpoint.com\\n...   \n",
       "9918   0011U00001S92XeQAJ  Additional To: harpreet_singh12@optum.com\\nCC:...   \n",
       "9919   0011U00001S91CEQAZ  Additional To: akumar5@bjs.com\\nCC: samyx@newr...   \n",
       "10484  0011U00001mXXwSQAW  https://app.salesloft.com/app/emails/detail/24...   \n",
       "...                   ...                                                ...   \n",
       "96146  0011U00001S8xXvQAJ  https://app.salesloft.com/app/emails/detail/24...   \n",
       "96175  0011U00001S8bvlQAB  Additional To: rmedida@opentext.com\\nCC: mcast...   \n",
       "96359  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
       "96360  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
       "96371  0011U00001S8hX9QAJ  Additional To: support-systems+newrelic2@acqui...   \n",
       "\n",
       "                                     SENDER CONVERSATION_ID  \\\n",
       "9916          jaiprasad.arsikere@alight.com      785d5e7a39   \n",
       "9917          felipe.braun@patientpoint.com      7926649e69   \n",
       "9918             harpreet_singh12@optum.com      b9787916e5   \n",
       "9919                        akumar5@bjs.com      4de2ce8f35   \n",
       "10484        chris.lewicki@meridianlink.com      bc7730854a   \n",
       "...                                     ...             ...   \n",
       "96146          jimmy.caroupapoulle@on24.com      9201a4f7a9   \n",
       "96175                  rmedida@opentext.com      563b0afab1   \n",
       "96359             john.stauffer@usfoods.com      279f03981e   \n",
       "96360             john.stauffer@usfoods.com      279f03981e   \n",
       "96371  support-systems+newrelic2@acquia.com      3dbdc58c08   \n",
       "\n",
       "                            CATEGORY  RESPONSE_TIME  \n",
       "9916         Support Case Management            NaN  \n",
       "9917         Support Case Management            NaN  \n",
       "9918         Support Case Management            NaN  \n",
       "9919         Support Case Management            NaN  \n",
       "10484        Support Case Management            NaN  \n",
       "...                              ...            ...  \n",
       "96146  Product Feedback and Strategy            NaN  \n",
       "96175             Account Management            NaN  \n",
       "96359        Support Case Management            NaN  \n",
       "96360             Account Management            NaN  \n",
       "96371        Support Case Management            NaN  \n",
       "\n",
       "[170 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "# )\n",
    "\n",
    "# # Constants\n",
    "# BATCH_SIZE = 500\n",
    "# LLM_MODEL = \"llama3.2:3b\"\n",
    "# CATEGORIES = [\n",
    "#     \"Support Case Management\",\n",
    "#     \"Technical Discussion\",\n",
    "#     \"Billing and Invoicing\", \n",
    "#     \"Contract and Renewal\",\n",
    "#     \"Meeting Scheduling\",\n",
    "#     \"Account Management\",\n",
    "#     \"Churn Risk Notification\",\n",
    "#     \"Event Management and Invitations\",\n",
    "#     \"Marketing Communications\",\n",
    "#     \"Gift Tracking\",\n",
    "#     \"Product Feedback and Strategy\",\n",
    "#     \"Brief Acknowledgments\",\n",
    "#     \"Call Logs\",\n",
    "#     \"Other\"\n",
    "# ]\n",
    "\n",
    "# def normalize_subject(subject):\n",
    "#     \"\"\"Normalize email subject line for better matching\"\"\"\n",
    "#     if pd.isna(subject) or subject is None:\n",
    "#         return \"\"\n",
    "#     subject = str(subject).strip()\n",
    "#     old_subject = \"\"\n",
    "#     while old_subject != subject:\n",
    "#         old_subject = subject\n",
    "#         subject = re.sub(r'^(re|fwd|fw|forward):\\s*', '', subject, flags=re.IGNORECASE)\n",
    "#     subject = re.sub(r'(email|mail):\\s*[<>]+\\s*', '', subject, flags=re.IGNORECASE)\n",
    "#     subject = re.sub(r'\\[(inbox|outbox|sent|draft|spam|trash|folder)\\]\\s*-?\\s*', '', subject, flags=re.IGNORECASE)\n",
    "#     subject = re.sub(r'\\[external\\]\\s*', '', subject, flags=re.IGNORECASE)\n",
    "#     subject = re.sub(r'^[^a-zA-Z0-9]+', '', subject)\n",
    "#     subject = re.sub(r'\\s+', ' ', subject).strip()\n",
    "#     return subject\n",
    "\n",
    "# def extract_top_response(text):\n",
    "#     \"\"\"Extract top response from email chains\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "        \n",
    "#     # Look for common patterns that indicate the beginning of a previous email\n",
    "#     separators = [\n",
    "#         r\"\\nFrom:.*?\\nSent:\", \n",
    "#         r\"\\nFrom:.*?\\n\",\n",
    "#         r\"\\nOn .*? wrote:\",\n",
    "#         r\"\\n-+\\s*Original Message\\s*-+\",\n",
    "#         r\"\\n_{3,}\",\n",
    "#         r\"\\nOn .* at .*:\"\n",
    "#     ]\n",
    "        \n",
    "#     # Join all patterns with OR\n",
    "#     combined_pattern = '|'.join(f'({pattern})' for pattern in separators)\n",
    "        \n",
    "#     # Find the first occurrence of any separator\n",
    "#     match = re.search(combined_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "#     if match:\n",
    "#         # Extract everything before the first separator\n",
    "#         first_part = text[:match.start()].strip()\n",
    "#         return first_part\n",
    "        \n",
    "#     # If no separator found, return the whole text\n",
    "#     return text.strip()\n",
    "\n",
    "# def extract_sender(text):\n",
    "#     \"\"\"Extract sender from emails\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return None\n",
    "        \n",
    "#     # Compile regex patterns for better performance\n",
    "#     email_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "#     from_pattern = re.compile(r\"From:\\s*(.*?)(?:\\[([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\]|<([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})>|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}))\")\n",
    "        \n",
    "#     # Find the first section (before any \"Original Message\" text)\n",
    "#     separator_pos = text.find(\"Original Message\")\n",
    "#     first_section = text[:separator_pos] if separator_pos > 0 else text\n",
    "        \n",
    "#     # Try to find a sender from \"From:\" line\n",
    "#     from_match = from_pattern.search(first_section)\n",
    "#     if from_match:\n",
    "#         # Return the first non-None group containing an email\n",
    "#         for group in from_match.groups()[1:]:\n",
    "#             if group:\n",
    "#                 return group.strip()\n",
    "                \n",
    "#         # Check if the name part contains an email\n",
    "#         name_part = from_match.group(1) if from_match.group(1) else \"\"\n",
    "#         email_in_name = email_pattern.search(name_part)\n",
    "#         if email_in_name:\n",
    "#             return email_in_name.group(0).strip()\n",
    "        \n",
    "#     # If no match in the \"From:\" line, search for any email address in the text\n",
    "#     email_match = email_pattern.search(first_section)\n",
    "#     if email_match:\n",
    "#         return email_match.group(0).strip()\n",
    "        \n",
    "#     return None\n",
    "\n",
    "# def generate_hashkey(text):\n",
    "#     \"\"\"Generate a hash key from text\"\"\"\n",
    "#     text = str(text).lower().strip()\n",
    "#     return hashlib.md5(text.encode()).hexdigest()[:10]\n",
    "\n",
    "# def generate_conversation_id(row):\n",
    "#     \"\"\"Generate a conversation ID based on subject and account info\"\"\"\n",
    "#     subject = normalize_subject(row.get('SUBJECT', \"\"))\n",
    "#     account_id = str(row.get('ACCOUNT_ID', \"unknown\"))\n",
    "#     base_data = f\"{account_id}-\"\n",
    "    \n",
    "#     # Try to extract organization name and ID from subject\n",
    "#     core_topic_match = re.search(r'([A-Za-z-]+)\\s*[\\(\\[]?(\\d+)[\\)\\]]?', subject)\n",
    "#     if core_topic_match:\n",
    "#         org_name = core_topic_match.group(1).strip()\n",
    "#         id_number = core_topic_match.group(2).strip()\n",
    "#         topic_key = f\"{base_data}{org_name.lower()}-{id_number}\"\n",
    "#         return generate_hashkey(topic_key)\n",
    "    \n",
    "#     # Check for meeting patterns\n",
    "#     meeting_patterns = [\n",
    "#         r'(invitation|accepted|declined|tentative).*?(\\w+\\s*-\\s*\\w+)',\n",
    "#         r'(meeting|call|conference|sync).*?(\\w+\\s*-\\s*\\w+)',\n",
    "#         r'(zoom|teams|webex|google\\s*meet).*?(\\w+\\s*-\\s*\\w+)'\n",
    "#     ]\n",
    "#     for pattern in meeting_patterns:\n",
    "#         meeting_match = re.search(pattern, subject, re.IGNORECASE)\n",
    "#         if meeting_match:\n",
    "#             meeting_name = meeting_match.group(2).strip().lower()\n",
    "#             return generate_hashkey(f\"{base_data}meeting-{meeting_name}\")\n",
    "    \n",
    "#     # If subject exists, use it\n",
    "#     if subject:\n",
    "#         return generate_hashkey(f\"{base_data}{subject.lower()}\")\n",
    "    \n",
    "#     # If TOP_RESPONSE exists, use first 50 chars\n",
    "#     if 'TOP_RESPONSE' in row and not pd.isna(row['TOP_RESPONSE']):\n",
    "#         text_for_hash = f\"{base_data}{str(row['TOP_RESPONSE'])[:50].lower()}\"\n",
    "#         return generate_hashkey(text_for_hash)\n",
    "    \n",
    "#     # Final fallback - use activity date\n",
    "#     fallback_text = f\"{base_data}{str(row.get('ACTIVITY_DATE', ''))}\"\n",
    "#     return generate_hashkey(fallback_text)\n",
    "\n",
    "# def calculate_response_time(df):\n",
    "#     \"\"\"Calculate response time between messages with account context\"\"\"\n",
    "#     df_new = df.copy()\n",
    "    \n",
    "#     # Ensure RESPONSE_TIME column exists\n",
    "#     if 'RESPONSE_TIME' not in df_new.columns:\n",
    "#         df_new['RESPONSE_TIME'] = np.nan\n",
    "    \n",
    "#     # Check if required columns exist for calculation\n",
    "#     required_cols = ['ACCOUNT_ID', 'CONVERSATION_ID', 'CONVERSATION_POSITION', 'SENDER', 'ACTIVITY_DATE']\n",
    "#     if not all(col in df_new.columns for col in required_cols):\n",
    "#         logging.warning(f\"Missing required columns for response time calculation. Have: {list(df_new.columns)}\")\n",
    "#         return df_new\n",
    "    \n",
    "#     df_new = df_new.sort_values(['ACCOUNT_ID', 'CONVERSATION_ID', 'CONVERSATION_POSITION'])\n",
    "    \n",
    "#     for (account_id, conv_id), group in df_new.groupby(['ACCOUNT_ID', 'CONVERSATION_ID']):\n",
    "#         if len(group) <= 1:\n",
    "#             continue\n",
    "            \n",
    "#         indices = group.index\n",
    "#         positions = group['CONVERSATION_POSITION'].tolist()\n",
    "#         senders = group['SENDER'].tolist()\n",
    "#         dates = group['ACTIVITY_DATE'].tolist()\n",
    "        \n",
    "#         for i in range(1, len(positions)):\n",
    "#             if pd.notna(senders[i]) and pd.notna(senders[i-1]) and senders[i] != senders[i-1]:\n",
    "#                 # Check if dates are datetime objects before calculating difference\n",
    "#                 if pd.notna(dates[i]) and pd.notna(dates[i-1]) and isinstance(dates[i], (pd.Timestamp, datetime)) and isinstance(dates[i-1], (pd.Timestamp, datetime)):\n",
    "#                     time_diff = (dates[i] - dates[i-1]).total_seconds() / 3600\n",
    "#                     idx = indices[i]\n",
    "#                     df_new.at[idx, 'RESPONSE_TIME'] = time_diff\n",
    "    \n",
    "#     return df_new\n",
    "\n",
    "# def categorize_email_with_llm(batch_df, batch_num=0):\n",
    "#     \"\"\"\n",
    "#     Categorize a batch of emails using the LLM model\n",
    "    \n",
    "#     Args:\n",
    "#         batch_df (pandas.DataFrame): Batch of emails to categorize\n",
    "#         batch_num (int): Batch number for reporting\n",
    "        \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: Dataframe with categorized emails\n",
    "#     \"\"\"\n",
    "#     result_df = batch_df.copy()\n",
    "    \n",
    "#     # Ensure CATEGORY column exists\n",
    "#     if 'CATEGORY' not in result_df.columns:\n",
    "#         result_df['CATEGORY'] = None\n",
    "    \n",
    "#     # Create a list to store emails categorized as \"Other\"\n",
    "#     others = []\n",
    "    \n",
    "#     # Process each email in the batch\n",
    "#     logging.info(f\"Categorizing batch {batch_num+1} with {len(batch_df)} emails...\")\n",
    "#     for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df)):\n",
    "#         # Get the email content\n",
    "#         content = row['TOP_RESPONSE'] if 'TOP_RESPONSE' in row and not pd.isna(row['TOP_RESPONSE']) else \"\"\n",
    "#         subject = row['SUBJECT'] if 'SUBJECT' in row and not pd.isna(row['SUBJECT']) else \"\"\n",
    "        \n",
    "#         # Clean content for better categorization\n",
    "#         if isinstance(content, str):\n",
    "#             lines = content.split('\\n')\n",
    "#             clean_lines = []\n",
    "#             skip_line = False\n",
    "            \n",
    "#             for line in lines:\n",
    "#                 # Skip lines with common email metadata patterns\n",
    "#                 if (re.match(r'^\\s*(From|To|Cc|Bcc|Date|Sent|Subject|Reply-To):', line, re.IGNORECASE) or\n",
    "#                     re.match(r'^\\s*On .* wrote:', line, re.IGNORECASE) or\n",
    "#                     re.match(r'^\\s*-+\\s*Original Message\\s*-+', line, re.IGNORECASE) or\n",
    "#                     re.match(r'^\\s*-{3,}$', line)):\n",
    "#                     skip_line = True\n",
    "#                     continue\n",
    "                    \n",
    "#                 # If we see a blank line after metadata, start including content again\n",
    "#                 if skip_line and not line.strip():\n",
    "#                     skip_line = False\n",
    "                    \n",
    "#                 if not skip_line:\n",
    "#                     clean_lines.append(line)\n",
    "            \n",
    "#             # Join back into a clean text\n",
    "#             clean_content = '\\n'.join(clean_lines).strip()\n",
    "#         else:\n",
    "#             clean_content = \"\"\n",
    "            \n",
    "#         # Normalize the subject\n",
    "#         normalized_subject = normalize_subject(subject)\n",
    "        \n",
    "#         # Create the prompt for the LLM\n",
    "#         prompt = f\"\"\"\n",
    "#         Task: You are categorizing business emails.\n",
    "        \n",
    "#         Categories (select ONLY ONE from this list):\n",
    "#         Support Case Management, Technical Discussion, Billing and Invoicing, Contract and Renewal, Meeting Scheduling, Account Management, Churn Risk Notification, \n",
    "#         Event Management and Invitations, Marketing Communications, Gift Tracking, Product Feedback and Strategy, Brief Acknowledgments, Call Logs, Other\n",
    "        \n",
    "#         Email subject: {normalized_subject}\n",
    "        \n",
    "#         Email content:\n",
    "#         {clean_content[:1000]}\n",
    "        \n",
    "#         IMPORTANT: Output ONLY ONE category name from the list above.\n",
    "#         Do not output any other text, explanation, or create new categories.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Make API call to Ollama with retries\n",
    "#         max_retries = 3\n",
    "#         retry_delay = 2  # seconds\n",
    "#         category = None\n",
    "        \n",
    "#         for attempt in range(max_retries):\n",
    "#             try:\n",
    "#                 response = requests.post(\n",
    "#                     'http://localhost:11434/api/generate',\n",
    "#                     json={\n",
    "#                         'model': LLM_MODEL,\n",
    "#                         'prompt': prompt,\n",
    "#                         'stream': False,\n",
    "#                         'temperature': 0.1\n",
    "#                     },\n",
    "#                     timeout=30\n",
    "#                 )\n",
    "                \n",
    "#                 if response.status_code == 200:\n",
    "#                     result = response.json()\n",
    "#                     response_text = result.get('response', '').strip()\n",
    "                    \n",
    "#                     # Normalize and match category\n",
    "#                     response_lower = response_text.lower()\n",
    "                    \n",
    "#                     # Check for exact matches first\n",
    "#                     matched = False\n",
    "#                     for category_name in CATEGORIES:\n",
    "#                         if category_name.lower() == response_lower:\n",
    "#                             category = category_name\n",
    "#                             matched = True\n",
    "#                             break\n",
    "                            \n",
    "#                     # If no exact match, check for partial matches\n",
    "#                     if not matched:\n",
    "#                         for category_name in CATEGORIES:\n",
    "#                             if category_name.lower() in response_lower:\n",
    "#                                 category = category_name\n",
    "#                                 matched = True\n",
    "#                                 break\n",
    "                    \n",
    "#                     # If still no match, default to \"Other\"\n",
    "#                     if not matched:\n",
    "#                         category = \"Other\"\n",
    "#                         logging.info(f\"LLM response '{response_text}' didn't match any category, defaulting to 'Other'\")\n",
    "                    \n",
    "#                     # Add to \"Other\" list ONLY if categorized as \"Other\"\n",
    "#                     if category == \"Other\":\n",
    "#                         others.append({\n",
    "#                             'SUBJECT': subject,\n",
    "#                             'TOP_RESPONSE': content[:500] if isinstance(content, str) else \"\",\n",
    "#                             'LLM_RESPONSE': response_text\n",
    "#                         })\n",
    "                    \n",
    "#                     break  # Success, exit retry loop\n",
    "#                 else:\n",
    "#                     logging.warning(f\"API error: {response.status_code}, attempt {attempt+1}/{max_retries}\")\n",
    "#                     if attempt < max_retries - 1:\n",
    "#                         time.sleep(retry_delay)\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"Error calling Ollama API: {str(e)}, attempt {attempt+1}/{max_retries}\")\n",
    "#                 if attempt < max_retries - 1:\n",
    "#                     time.sleep(retry_delay)\n",
    "        \n",
    "#         # If all retries failed, default to \"Other\"\n",
    "#         if category is None:\n",
    "#             category = \"Other\"\n",
    "#             logging.warning(f\"All API calls failed for email with subject '{subject[:30]}...', defaulting to 'Other'\")\n",
    "            \n",
    "#             # Add to \"Other\" list\n",
    "#             others.append({\n",
    "#                 'SUBJECT': subject,\n",
    "#                 'TOP_RESPONSE': content[:500] if isinstance(content, str) else \"\",\n",
    "#                 'LLM_RESPONSE': \"API Error\"\n",
    "#             })\n",
    "        \n",
    "#         # Set the category in the result dataframe\n",
    "#         result_df.at[idx, 'CATEGORY'] = category\n",
    "    \n",
    "#     # Save \"Other\" categories for this batch\n",
    "#     if others:\n",
    "#         others_df = pd.DataFrame(others)\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         filename = f'other_categories_batch_{batch_num+1}_{timestamp}.csv'\n",
    "#         others_df.to_csv(filename, index=False)\n",
    "#         logging.info(f\"Saved {len(others)} 'Other' categorizations to {filename}\")\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "# def preprocess_emails(df):\n",
    "#     \"\"\"Preprocess emails to extract TOP_RESPONSE and SENDER if needed\"\"\"\n",
    "#     df_new = df.copy()\n",
    "    \n",
    "#     # Ensure required columns exist\n",
    "#     for col in ['TOP_RESPONSE', 'SENDER']:\n",
    "#         if col not in df_new.columns:\n",
    "#             df_new[col] = None\n",
    "#             logging.info(f\"Created column '{col}' in the dataframe\")\n",
    "    \n",
    "#     # Process each row\n",
    "#     for idx, row in tqdm(df_new.iterrows(), total=len(df_new)):\n",
    "#         # Extract TOP_RESPONSE if not already set\n",
    "#         if pd.isna(df_new.at[idx, 'TOP_RESPONSE']) or df_new.at[idx, 'TOP_RESPONSE'] == '':\n",
    "#             if 'DESCRIPTION' in row and not pd.isna(row['DESCRIPTION']):\n",
    "#                 content = row['DESCRIPTION']\n",
    "#                 top_response = extract_top_response(content)\n",
    "#                 df_new.at[idx, 'TOP_RESPONSE'] = top_response\n",
    "        \n",
    "#         # Extract SENDER if not already set\n",
    "#         if pd.isna(df_new.at[idx, 'SENDER']) or df_new.at[idx, 'SENDER'] == '':\n",
    "#             if not pd.isna(df_new.at[idx, 'TOP_RESPONSE']):\n",
    "#                 sender = extract_sender(df_new.at[idx, 'TOP_RESPONSE'])\n",
    "#                 df_new.at[idx, 'SENDER'] = sender.lower() if sender else None\n",
    "    \n",
    "#     return df_new\n",
    "\n",
    "# def process_emails_in_batches(df_input, num_batches=5):\n",
    "#     \"\"\"\n",
    "#     Process emails in batches, categorizing only a specified number of batches with LLM\n",
    "    \n",
    "#     Args:\n",
    "#         df_input (pandas.DataFrame): Input dataframe with email data\n",
    "#         num_batches (int): Number of batches to process with LLM\n",
    "        \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: Processed dataframe with additional columns\n",
    "#     \"\"\"\n",
    "#     df = df_input.copy()\n",
    "    \n",
    "#     # Step 1: Preprocess to extract TOP_RESPONSE and SENDER\n",
    "#     logging.info(\"Step 1: Preprocessing emails...\")\n",
    "#     df = preprocess_emails(df)\n",
    "    \n",
    "#     # Step 2: Add CONVERSATION_ID\n",
    "#     logging.info(\"Step 2: Adding conversation IDs...\")\n",
    "    \n",
    "#     # Initialize required columns if they don't exist\n",
    "#     for col in ['CONVERSATION_ID', 'CATEGORY']:\n",
    "#         if col not in df.columns:\n",
    "#             df[col] = None\n",
    "#             logging.info(f\"Created column '{col}' in the dataframe\")\n",
    "    \n",
    "#     # Add conversation ID\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating conversation IDs\"):\n",
    "#         df.at[idx, 'CONVERSATION_ID'] = generate_conversation_id(row)\n",
    "    \n",
    "#     # Step 3: Split into batches randomly\n",
    "#     logging.info(f\"Step 3: Splitting into batches for categorization ({num_batches} batches will be processed)...\")\n",
    "    \n",
    "#     # Shuffle and split\n",
    "#     df_shuffled = df.sample(frac=1, random_state=42)  # Shuffle with fixed random seed\n",
    "#     batches = [df_shuffled.iloc[i:i+BATCH_SIZE] for i in range(0, len(df_shuffled), BATCH_SIZE)]\n",
    "    \n",
    "#     # Step 4: Process specified number of batches\n",
    "#     logging.info(f\"Step 4: Processing {min(num_batches, len(batches))} batches with LLM...\")\n",
    "    \n",
    "#     for i in range(min(num_batches, len(batches))):\n",
    "#         batch_df = batches[i]\n",
    "        \n",
    "#         # Categorize emails in the batch\n",
    "#         categorized_batch = categorize_email_with_llm(batch_df, i)\n",
    "#         # We don't update the main dataframe since we're only interested in the \"Other\" files\n",
    "    \n",
    "#     # Report completed\n",
    "#     logging.info(f\"Processing complete! Processed {min(num_batches, len(batches))} batches.\")\n",
    "    \n",
    "#     # We don't need to return the dataframe since we're only saving the \"Other\" files\n",
    "#     return None\n",
    "\n",
    "# # %%\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     try:\n",
    "#         # Access the tasks_df from the global scope\n",
    "#         global tasks_df\n",
    "#         if 'tasks_df' not in globals():\n",
    "#             logging.error(\"tasks_df not found. Please load the dataframe before running.\")\n",
    "#             return None\n",
    "        \n",
    "#         # Display basic information about the dataframe\n",
    "#         logging.info(f\"Processing dataframe with {len(tasks_df)} rows and columns: {list(tasks_df.columns)}\")\n",
    "        \n",
    "#         # Process the emails (we don't need to store the return value)\n",
    "#         process_emails_in_batches(tasks_df, num_batches=5)\n",
    "        \n",
    "#         # No need to save the main results file\n",
    "#         logging.info(\"Processing complete. 'Other' category files have been saved.\")\n",
    "        \n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error in main function: {str(e)}\")\n",
    "#         import traceback\n",
    "#         logging.error(traceback.format_exc())\n",
    "#         return None\n",
    "\n",
    "# # %%\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 CSV files to combine:\n",
      "  - processed_emails_intermediate_20250404_200232.csv\n",
      "  - processed_emails_intermediate_20250404_164724.csv\n",
      "  - processed_emails_intermediate_20250405_184509.csv\n",
      "  - processed_emails_intermediate_20250404_192927.csv\n",
      "  - processed_emails_intermediate_20250405_192634.csv\n",
      "  - processed_emails_intermediate_20250404_172119.csv\n",
      "  - processed_emails_intermediate_20250404_182228.csv\n",
      "  - processed_emails_intermediate_20250405_181607.csv\n",
      "  - processed_emails_intermediate_20250404_120711.csv\n",
      "  - processed_emails_intermediate_20250404_211011.csv\n",
      "  - processed_emails_intermediate_20250404_185557.csv\n",
      "  - processed_emails_intermediate_20250405_174703.csv\n",
      "  - processed_emails_intermediate_20250405_191407.csv\n",
      "  - processed_emails_intermediate_20250404_203552.csv\n",
      "  - processed_emails_intermediate_20250405_171748.csv\n",
      "  - processed_emails_intermediate_20250404_124134.csv\n",
      "  - processed_emails_intermediate_20250404_150529.csv\n",
      "  - processed_emails_intermediate_20250404_161337.csv\n",
      "  - processed_emails_intermediate_20250404_154004.csv\n",
      "  - processed_emails_intermediate_20250404_214329.csv\n",
      "Successfully read processed_emails_intermediate_20250404_200232.csv with 20000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_164724.csv with 20000 rows\n",
      "Successfully read processed_emails_intermediate_20250405_184509.csv with 20000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_192927.csv with 15000 rows\n",
      "Successfully read processed_emails_intermediate_20250405_192634.csv with 27182 rows\n",
      "Successfully read processed_emails_intermediate_20250404_172119.csv with 25000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_182228.csv with 5000 rows\n",
      "Successfully read processed_emails_intermediate_20250405_181607.csv with 15000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_120711.csv with 5000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_211011.csv with 139173 rows\n",
      "Successfully read processed_emails_intermediate_20250404_185557.csv with 10000 rows\n",
      "Successfully read processed_emails_intermediate_20250405_174703.csv with 10000 rows\n",
      "Successfully read processed_emails_intermediate_20250405_191407.csv with 25000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_203552.csv with 134173 rows\n",
      "Successfully read processed_emails_intermediate_20250405_171748.csv with 5000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_124134.csv with 10000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_150529.csv with 5000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_161337.csv with 15000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_154004.csv with 10000 rows\n",
      "Successfully read processed_emails_intermediate_20250404_214329.csv with 144173 rows\n",
      "\n",
      "Combined dataframe has 659701 rows and 9 columns\n",
      "\n",
      "Combined data saved to combined_processed_emails.csv\n",
      "\n",
      "Combined DataFrame Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 659701 entries, 0 to 659700\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   SUBJECT          528793 non-null  object\n",
      " 1   DESCRIPTION      528790 non-null  object\n",
      " 2   ACTIVITY_DATE    528793 non-null  object\n",
      " 3   ACCOUNT_ID       528793 non-null  object\n",
      " 4   TOP_RESPONSE     528790 non-null  object\n",
      " 5   SENDER           512127 non-null  object\n",
      " 6   CONVERSATION_ID  528787 non-null  object\n",
      " 7   CATEGORY         528787 non-null  object\n",
      " 8   source_file      659701 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 45.3+ MB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "                                             SUBJECT  \\\n",
      "0  Email: New Relic support case comment: Case #0...   \n",
      "1  Email: New Relic support case comment: Case #0...   \n",
      "2  Email: New Relic support case comment: Case #0...   \n",
      "3                   Call: Left Voicemail | No Answer   \n",
      "4  Email: Re: New Relic support case created: Cas...   \n",
      "\n",
      "                                         DESCRIPTION ACTIVITY_DATE  \\\n",
      "0  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-04   \n",
      "1  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-04   \n",
      "2  Additional To: jesus.garcia@sopristec.com\\nCC:...    2025-04-04   \n",
      "3  N/A, Left Vm \\n\\nRecording: https://recordings...    2025-04-04   \n",
      "4  Additional To: serena.xu@pantheon.io\\nCC: \\nBC...    2025-04-04   \n",
      "\n",
      "           ACCOUNT_ID                                       TOP_RESPONSE  \\\n",
      "0  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
      "1  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
      "2  0011U00001aUhSFQA0  Additional To: jesus.garcia@sopristec.com\\nCC:...   \n",
      "3  0011U00001S8nshQAB  N/A, Left Vm \\n\\nRecording: https://recordings...   \n",
      "4  0011U00001S8iBfQAJ  Additional To: serena.xu@pantheon.io\\nCC: \\nBC...   \n",
      "\n",
      "                       SENDER CONVERSATION_ID                 CATEGORY  \\\n",
      "0   john.stauffer@usfoods.com      279f03981e  Support Case Management   \n",
      "1   john.stauffer@usfoods.com      c78b829888  Support Case Management   \n",
      "2  jesus.garcia@sopristec.com      80c283aa97  Support Case Management   \n",
      "3                         NaN      a3bb9fb841                Call Logs   \n",
      "4       serena.xu@pantheon.io      f170d0ee68  Support Case Management   \n",
      "\n",
      "                                         source_file  \n",
      "0  processed_emails_intermediate_20250404_200232.csv  \n",
      "1  processed_emails_intermediate_20250404_200232.csv  \n",
      "2  processed_emails_intermediate_20250404_200232.csv  \n",
      "3  processed_emails_intermediate_20250404_200232.csv  \n",
      "4  processed_emails_intermediate_20250404_200232.csv  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the path where your CSV files are located\n",
    "# Replace with your actual path if different\n",
    "csv_path = './'  # Current directory\n",
    "\n",
    "# Use glob to get all CSV files in the directory\n",
    "all_csv_files = glob.glob(os.path.join(csv_path, 'processed_emails_intermediate_*.csv'))\n",
    "\n",
    "# Print the files that will be combined\n",
    "print(f\"Found {len(all_csv_files)} CSV files to combine:\")\n",
    "for file in all_csv_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Function to combine all CSV files\n",
    "def combine_csv_files(file_list):\n",
    "    # Create an empty list to store individual dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    # Loop through all files and read them into dataframes\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Add source filename as a column (optional)\n",
    "            df['source_file'] = os.path.basename(file)\n",
    "            df_list.append(df)\n",
    "            print(f\"Successfully read {os.path.basename(file)} with {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\nCombined dataframe has {len(combined_df)} rows and {len(combined_df.columns)} columns\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No dataframes to combine\")\n",
    "        return None\n",
    "\n",
    "# Combine the CSV files\n",
    "combined_df = combine_csv_files(all_csv_files)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "if combined_df is not None:\n",
    "    output_file = 'combined_processed_emails.csv'\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nCombined data saved to {output_file}\")\n",
    "    \n",
    "    # Display a summary of the combined data\n",
    "    print(\"\\nCombined DataFrame Summary:\")\n",
    "    print(combined_df.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>ACTIVITY_DATE</th>\n",
       "      <th>ACCOUNT_ID</th>\n",
       "      <th>TOP_RESPONSE</th>\n",
       "      <th>SENDER</th>\n",
       "      <th>CONVERSATION_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email: New Relic support case comment: Case #0...</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0011U00001S8XGAQA3</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>john.stauffer@usfoods.com</td>\n",
       "      <td>279f03981e</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>processed_emails_intermediate_20250404_200232.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Email: New Relic support case comment: Case #0...</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0011U00001S8XGAQA3</td>\n",
       "      <td>Additional To: john.stauffer@usfoods.com\\nCC: ...</td>\n",
       "      <td>john.stauffer@usfoods.com</td>\n",
       "      <td>c78b829888</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>processed_emails_intermediate_20250404_200232.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Email: New Relic support case comment: Case #0...</td>\n",
       "      <td>Additional To: jesus.garcia@sopristec.com\\nCC:...</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0011U00001aUhSFQA0</td>\n",
       "      <td>Additional To: jesus.garcia@sopristec.com\\nCC:...</td>\n",
       "      <td>jesus.garcia@sopristec.com</td>\n",
       "      <td>80c283aa97</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>processed_emails_intermediate_20250404_200232.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Call: Left Voicemail | No Answer</td>\n",
       "      <td>N/A, Left Vm \\n\\nRecording: https://recordings...</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0011U00001S8nshQAB</td>\n",
       "      <td>N/A, Left Vm \\n\\nRecording: https://recordings...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a3bb9fb841</td>\n",
       "      <td>Call Logs</td>\n",
       "      <td>processed_emails_intermediate_20250404_200232.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email: Re: New Relic support case created: Cas...</td>\n",
       "      <td>Additional To: serena.xu@pantheon.io\\nCC: \\nBC...</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0011U00001S8iBfQAJ</td>\n",
       "      <td>Additional To: serena.xu@pantheon.io\\nCC: \\nBC...</td>\n",
       "      <td>serena.xu@pantheon.io</td>\n",
       "      <td>f170d0ee68</td>\n",
       "      <td>Support Case Management</td>\n",
       "      <td>processed_emails_intermediate_20250404_200232.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             SUBJECT  \\\n",
       "0  Email: New Relic support case comment: Case #0...   \n",
       "1  Email: New Relic support case comment: Case #0...   \n",
       "2  Email: New Relic support case comment: Case #0...   \n",
       "3                   Call: Left Voicemail | No Answer   \n",
       "4  Email: Re: New Relic support case created: Cas...   \n",
       "\n",
       "                                         DESCRIPTION ACTIVITY_DATE  \\\n",
       "0  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-04   \n",
       "1  Additional To: john.stauffer@usfoods.com\\nCC: ...    2025-04-04   \n",
       "2  Additional To: jesus.garcia@sopristec.com\\nCC:...    2025-04-04   \n",
       "3  N/A, Left Vm \\n\\nRecording: https://recordings...    2025-04-04   \n",
       "4  Additional To: serena.xu@pantheon.io\\nCC: \\nBC...    2025-04-04   \n",
       "\n",
       "           ACCOUNT_ID                                       TOP_RESPONSE  \\\n",
       "0  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
       "1  0011U00001S8XGAQA3  Additional To: john.stauffer@usfoods.com\\nCC: ...   \n",
       "2  0011U00001aUhSFQA0  Additional To: jesus.garcia@sopristec.com\\nCC:...   \n",
       "3  0011U00001S8nshQAB  N/A, Left Vm \\n\\nRecording: https://recordings...   \n",
       "4  0011U00001S8iBfQAJ  Additional To: serena.xu@pantheon.io\\nCC: \\nBC...   \n",
       "\n",
       "                       SENDER CONVERSATION_ID                 CATEGORY  \\\n",
       "0   john.stauffer@usfoods.com      279f03981e  Support Case Management   \n",
       "1   john.stauffer@usfoods.com      c78b829888  Support Case Management   \n",
       "2  jesus.garcia@sopristec.com      80c283aa97  Support Case Management   \n",
       "3                         NaN      a3bb9fb841                Call Logs   \n",
       "4       serena.xu@pantheon.io      f170d0ee68  Support Case Management   \n",
       "\n",
       "                                         source_file  \n",
       "0  processed_emails_intermediate_20250404_200232.csv  \n",
       "1  processed_emails_intermediate_20250404_200232.csv  \n",
       "2  processed_emails_intermediate_20250404_200232.csv  \n",
       "3  processed_emails_intermediate_20250404_200232.csv  \n",
       "4  processed_emails_intermediate_20250404_200232.csv  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.read_csv('combined_processed_emails.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(columns=['CONVERSATION_ID', 'source_file'], inplace=True)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def generate_hash_key(row, key_columns):\n",
    "    \"\"\"Generate a unique hash key from multiple columns\"\"\"\n",
    "    values = []\n",
    "    for col in key_columns:\n",
    "        if col in row and not pd.isna(row[col]):\n",
    "            # Handle different data types appropriately\n",
    "            if hasattr(row[col], 'strftime'):  # For datetime objects\n",
    "                values.append(str(row[col].strftime('%Y-%m-%d %H:%M:%S.%f')))\n",
    "            else:\n",
    "                values.append(str(row[col]))\n",
    "        else:\n",
    "            values.append('NULL')\n",
    "    \n",
    "    # Join all values and hash\n",
    "    combined = '|'.join(values)\n",
    "    return hashlib.md5(combined.encode()).hexdigest()\n",
    "\n",
    "def deduplicate_df(df, output_file):\n",
    "    \"\"\"Deduplicate a dataframe based on key columns\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    original_count = len(df)\n",
    "    logging.info(f\"Working with DataFrame containing {original_count} rows\")\n",
    "    \n",
    "    # Convert date columns to datetime if needed\n",
    "    date_columns = ['ACTIVITY_DATE']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                logging.info(f\"Converted {col} to datetime\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not convert {col} to datetime: {str(e)}\")\n",
    "    \n",
    "    # Define key columns for deduplication\n",
    "    # Adjust these columns based on what uniquely identifies a row in your data\n",
    "    key_columns = ['ACCOUNT_ID', 'ACTIVITY_DATE', 'SUBJECT', 'DESCRIPTION']\n",
    "    \n",
    "    # Verify key columns exist\n",
    "    missing_columns = [col for col in key_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"Missing key columns: {missing_columns}\")\n",
    "        # Try to use alternative columns\n",
    "        available_columns = list(df.columns)\n",
    "        logging.info(f\"Available columns: {available_columns}\")\n",
    "        # Adjust key_columns to use only available columns\n",
    "        key_columns = [col for col in key_columns if col in df.columns]\n",
    "        if len(key_columns) == 0:\n",
    "            logging.error(\"No key columns available for deduplication\")\n",
    "            return None\n",
    "        logging.info(f\"Using alternative key columns: {key_columns}\")\n",
    "    \n",
    "    # Generate hash keys for each row\n",
    "    logging.info(\"Generating hash keys...\")\n",
    "    hash_keys = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating hash keys\"):\n",
    "        hash_key = generate_hash_key(row, key_columns)\n",
    "        hash_keys.append(hash_key)\n",
    "    \n",
    "    df['hash_key'] = hash_keys\n",
    "    \n",
    "    # Count occurrences of each hash key\n",
    "    hash_counts = df['hash_key'].value_counts()\n",
    "    duplicate_keys = hash_counts[hash_counts > 1].index.tolist()\n",
    "    \n",
    "    if duplicate_keys:\n",
    "        logging.info(f\"Found {len(duplicate_keys)} duplicate keys affecting {sum(hash_counts[duplicate_keys] - 1)} rows\")\n",
    "        # Display a few examples of duplicates\n",
    "        for key in duplicate_keys[:3]:\n",
    "            dupe_rows = df[df['hash_key'] == key]\n",
    "            logging.info(f\"Example duplicate (key {key}):\")\n",
    "            for col in key_columns:\n",
    "                if col in dupe_rows.columns:\n",
    "                    values = dupe_rows[col].unique()\n",
    "                    logging.info(f\"  {col}: {values}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_deduped = df.drop_duplicates(subset=['hash_key'])\n",
    "    \n",
    "    # Log results\n",
    "    deduped_count = len(df_deduped)\n",
    "    logging.info(f\"After deduplication: {deduped_count} rows (removed {original_count - deduped_count} duplicates)\")\n",
    "    \n",
    "    # Remove hash key column before saving\n",
    "    df_deduped = df_deduped.drop(columns=['hash_key'])\n",
    "    \n",
    "    # Save deduplicated data\n",
    "    df_deduped.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Deduplicated data saved to {output_file}\")\n",
    "    \n",
    "    return df_deduped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use it like this:\n",
    "output_file = \"deduplicated_df.csv\"\n",
    "deduplicated_df = deduplicate_df(combined_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97186 entries, 0 to 97185\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   SUBJECT        97185 non-null  object\n",
      " 1   DESCRIPTION    97184 non-null  object\n",
      " 2   ACTIVITY_DATE  97185 non-null  object\n",
      " 3   ACCOUNT_ID     97185 non-null  object\n",
      " 4   TOP_RESPONSE   97184 non-null  object\n",
      " 5   SENDER         92328 non-null  object\n",
      " 6   CATEGORY       97183 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dedup_df = pd.read_csv('deduplicated_df.csv')\n",
    "dedup_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df_copy = dedup_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_dataframe(df):\n",
    "    \"\"\"Apply all conversation analysis functions to the dataframe\"\"\"\n",
    "    # Generate conversation IDs\n",
    "    df['CONVERSATION_ID'] = df.apply(generate_conversation_id, axis=1)\n",
    "    \n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert ACTIVITY_DATE to datetime if it's not already, with error handling\n",
    "    try:\n",
    "        # First check if there are problematic values and print them for debugging\n",
    "        if df['ACTIVITY_DATE'].dtype == 'object':  # If it's a string or mixed type\n",
    "            non_date_values = df[~df['ACTIVITY_DATE'].astype(str).str.match(r'^\\d{4}-\\d{2}-\\d{2}.*$')]\n",
    "            if len(non_date_values) > 0:\n",
    "                print(f\"Found {len(non_date_values)} rows with problematic date values:\")\n",
    "                print(non_date_values['ACTIVITY_DATE'].head())\n",
    "                \n",
    "                # Replace problematic values with NaT (Not a Time)\n",
    "                df['ACTIVITY_DATE'] = pd.to_datetime(df['ACTIVITY_DATE'], errors='coerce')\n",
    "            else:\n",
    "                df['ACTIVITY_DATE'] = pd.to_datetime(df['ACTIVITY_DATE'])\n",
    "        else:\n",
    "            # Already datetime or easily convertible\n",
    "            df['ACTIVITY_DATE'] = pd.to_datetime(df['ACTIVITY_DATE'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dates: {e}\")\n",
    "        # Fallback: coerce invalid dates to NaT\n",
    "        df['ACTIVITY_DATE'] = pd.to_datetime(df['ACTIVITY_DATE'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    invalid_dates = df['ACTIVITY_DATE'].isna()\n",
    "    if invalid_dates.any():\n",
    "        print(f\"Dropping {invalid_dates.sum()} rows with invalid dates\")\n",
    "        df = df.dropna(subset=['ACTIVITY_DATE'])\n",
    "    \n",
    "    # Sort by account, conversation and date\n",
    "    df = df.sort_values(['ACCOUNT_ID', 'CONVERSATION_ID', 'ACTIVITY_DATE'])\n",
    "    \n",
    "    # Initialize or reset conversation positions\n",
    "    df['CONVERSATION_POSITION'] = 0\n",
    "    \n",
    "    # Set conversation positions\n",
    "    position_counter = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        key = (row['ACCOUNT_ID'], row['CONVERSATION_ID'])\n",
    "        if key not in position_counter:\n",
    "            position_counter[key] = 1  # Start from 1 instead of 0\n",
    "        else:\n",
    "            position_counter[key] += 1\n",
    "        df.at[idx, 'CONVERSATION_POSITION'] = position_counter[key]\n",
    "    \n",
    "    # Calculate response times\n",
    "    df = calculate_response_time(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 rows with problematic date values:\n",
      "79082             NaN\n",
      "79083     sin embargo\n",
      "79084      905cbc369c\n",
      "87185     sin embargo\n",
      "Name: ACTIVITY_DATE, dtype: object\n",
      "Dropping 4 rows with invalid dates\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97182 entries, 90197 to 33742\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   SUBJECT                97182 non-null  object        \n",
      " 1   DESCRIPTION            97182 non-null  object        \n",
      " 2   ACTIVITY_DATE          97182 non-null  datetime64[ns]\n",
      " 3   ACCOUNT_ID             97182 non-null  object        \n",
      " 4   TOP_RESPONSE           97182 non-null  object        \n",
      " 5   SENDER                 92326 non-null  object        \n",
      " 6   CATEGORY               97181 non-null  object        \n",
      " 7   CONVERSATION_ID        97182 non-null  object        \n",
      " 8   CONVERSATION_POSITION  97182 non-null  int64         \n",
      " 9   RESPONSE_TIME          4945 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(7)\n",
      "memory usage: 10.2+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Then run your original code\n",
    "dedup_resp_df = apply_to_dataframe(dedup_df_copy)\n",
    "dedup_resp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_resp_df.to_csv('processed_emails_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
